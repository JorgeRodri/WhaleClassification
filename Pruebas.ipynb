{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import aifc\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import mlab\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = \"C:\\\\Users\\\\jorge\\\\DatasetsTFM\\\\KaggleData\\\\train.csv\"\n",
    "train_path = \"C:\\\\Users\\\\jorge\\\\DatasetsTFM\\\\KaggleData\\\\train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\jorge\\\\DatasetsTFM\\\\KaggleData\\\\train\\\\train1.aiff',\n",
       " 'C:\\\\Users\\\\jorge\\\\DatasetsTFM\\\\KaggleData\\\\train\\\\train10.aiff',\n",
       " 'C:\\\\Users\\\\jorge\\\\DatasetsTFM\\\\KaggleData\\\\train\\\\train100.aiff',\n",
       " 'C:\\\\Users\\\\jorge\\\\DatasetsTFM\\\\KaggleData\\\\train\\\\train1000.aiff',\n",
       " 'C:\\\\Users\\\\jorge\\\\DatasetsTFM\\\\KaggleData\\\\train\\\\train10000.aiff']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiofiles = [os.path.join(train_path, f) for f in listdir(train_path) if isfile(join(train_path, f))]\n",
    "audiofiles[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "f = aifc.open(audiofiles[0])\n",
    "print(f.getframerate())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jorge\\pycharmprojects\\audioextraction\\tfenv\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def ReadAIFF(file):\n",
    "    s = aifc.open(file,'r')\n",
    "    nFrames = s.getnframes()\n",
    "    strSig = s.readframes(nFrames)\n",
    "    return np.fromstring(strSig, np.short).byteswap()\n",
    "\n",
    "s = ReadAIFF(audiofiles[0])\n",
    "\n",
    "params = {'NFFT':256, 'Fs':2000, 'noverlap':192}\n",
    "\n",
    "P, freqs, bins = mlab.specgram(s, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 59)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train1.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train2.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train3.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train4.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train5.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     clip_name  label\n",
       "0  train1.aiff      0\n",
       "1  train2.aiff      0\n",
       "2  train3.aiff      0\n",
       "3  train4.aiff      0\n",
       "4  train5.aiff      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv(labels_path)\n",
    "display(df.head())\n",
    "\n",
    "def get_labels(labels_path):\n",
    "    labels = dict()\n",
    "    with open(labels_path, 'r') as f:\n",
    "        reader = csv.reader(f, dialect='excel')\n",
    "        for row in reader:\n",
    "            labels[row[0]] = row[1]\n",
    "    return labels\n",
    "\n",
    "labels_dict = get_labels(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train1.aiff'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiofiles[0].split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2800,), (2800,), (2800,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.7\n",
    "s[:int(s.shape[0] * p)].shape, s[int(s.shape[0] * (1-p)/2): int(s.shape[0] * (1+p)/2)].shape, s[int(s.shape[0] * (1-p)):].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfunc = np.vectorize(lambda x: labels_dict[x.split('\\\\')[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22973, 7027)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vfunc(audiofiles) == '0').sum(), (vfunc(audiofiles) == '1').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21000,), (9000,), 4931, 16069)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(audiofiles)\n",
    "Y = vfunc(audiofiles)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "X_train.shape, X_test.shape, (Y_train=='1').sum(), (Y_train=='0').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jorge\\pycharmprojects\\audioextraction\\tfenv\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((9000, 129, 59), (9000,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_spects(onlyfiles, labels):\n",
    "    sps = []\n",
    "    y = []\n",
    "    for file_path in onlyfiles:\n",
    "        s = ReadAIFF(file_path)\n",
    "        y.append(labels[file_path.split(\"\\\\\")[-1]])\n",
    "        params = {'NFFT':256, 'Fs':2000, 'noverlap':192}\n",
    "        P, freqs, bins = mlab.specgram(s, **params)\n",
    "        sps.append(P)\n",
    "    return np.array(sps), np.array(y)\n",
    "\n",
    "X, Y = get_spects(X_test, labels_dict)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tamaño en memoria de los datos aprox: 1.70GB'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Tamaño en memoria de los datos aprox: %.2fGB' % (a.nbytes/2**10/2**10/2**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jorge\\pycharmprojects\\audioextraction\\tfenv\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90000, 40, 40), (90000,))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_spects_enhanced(onlyfiles, labels, p=0.7, cut= True):\n",
    "    if cut:\n",
    "        top_hz = 40\n",
    "    sps = []\n",
    "    y = []\n",
    "    for file_path in onlyfiles:\n",
    "        s = ReadAIFF(file_path)\n",
    "        this_label = int(labels[file_path.split(\"\\\\\")[-1]])\n",
    "        s1 = s[:int(s.shape[0] * p)]\n",
    "        y.append(this_label)\n",
    "        s2 = s[int(s.shape[0] * (1-p)/2): int(s.shape[0] * (1+p)/2)]\n",
    "        y.append(this_label)\n",
    "        s3 = s[int(s.shape[0] * (1-p)):]\n",
    "        y.append(this_label)\n",
    "        params = {'NFFT':256, 'Fs':2000, 'noverlap':192}\n",
    "        P1, freqs, bins = mlab.specgram(s1, **params)\n",
    "        sps.append(P1[:top_hz,:])\n",
    "        P2, freqs, bins = mlab.specgram(s2, **params)\n",
    "        sps.append(P2[:top_hz,:])\n",
    "        P3, freqs, bins = mlab.specgram(s3, **params)\n",
    "        sps.append(P3[:top_hz,:])\n",
    "    return np.array(sps), np.array(y)\n",
    "\n",
    "\n",
    "X, Y = get_spects_enhanced(audiofiles, labels_dict)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21081, 68919)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y=='1').sum(), (Y=='0').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21081, 40, 40), (21081,))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enhance_with_noise(X, Y, noise_p=0.28):\n",
    "    whale_index, = np.where(Y == '1')\n",
    "    no_whale_index, = np.where(Y == '0')\n",
    "    X_enhanced = []\n",
    "    Y_enhanced = []\n",
    "    for s_i in whale_index:\n",
    "        new_X = X[s_i] + noise_p*X[np.random.choice(no_whale_index)]\n",
    "        X_enhanced.append(new_X)\n",
    "        Y_enhanced.append(1)\n",
    "    return np.array(X_enhanced),  np.array(Y_enhanced)\n",
    "\n",
    "X_enhanced, Y_enhanced = enhance_with_noise(X, Y)\n",
    "X_enhanced.shape, Y_enhanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84081, 40, 40), (27000, 40, 40), 35996, 48085)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "X_train, Y_train = np.concatenate([X_train, X_enhanced]), np.concatenate([Y_train, Y_enhanced])\n",
    "X_train.shape, X_test.shape, (Y_train=='1').sum(), (Y_train=='0').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "84081/84081 [==============================] - 193s 2ms/sample - loss: 6.8600 - acc: 0.5719\n",
      "Epoch 2/5\n",
      "84081/84081 [==============================] - 195s 2ms/sample - loss: 6.8602 - acc: 0.5719\n",
      "Epoch 3/5\n",
      "84081/84081 [==============================] - 208s 2ms/sample - loss: 7.0057 - acc: 0.5621\n",
      "Epoch 4/5\n",
      "84081/84081 [==============================] - 217s 3ms/sample - loss: 6.8374 - acc: 0.5729\n",
      "Epoch 5/5\n",
      "84081/84081 [==============================] - 208s 2ms/sample - loss: 6.8539 - acc: 0.5723\n",
      "27000/27000 [==============================] - 20s 753us/sample - loss: 3.6606 - acc: 0.7716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.660609540515476, 0.77155554]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train, Y_test = Y_train.astype(int), Y_test.astype(int)\n",
    "Y_train, Y_test = tf.keras.utils.to_categorical(Y_train, 2), tf.keras.utils.to_categorical(Y_test, 2) \n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(20, kernel_size=(7, 7), activation=tf.nn.relu, input_shape=X_train.shape[1:], name='Conv1'),\n",
    "    tf.keras.layers.Dropout(0.2),  \n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(40, kernel_size=(7, 7), activation=tf.nn.relu, name='Conv2'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu, name=\"Dense1\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.softmax, name=\"Softmax\")\n",
    "    ])\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=5, verbose=1)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
